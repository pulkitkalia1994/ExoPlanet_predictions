---
title: "Predecting ExoPlanets"
author: "Pulkit Kalia"
date: "July 9, 2018"
output: html_document
---

#Predicting ExoPlanets using XGBoost

##Synopsis-
Our first glimpse at planets outside of the solar system we call home came in 1992 when several terrestrial-mass planets were detected orbiting the pulsar PSR B1257+12. In this dataset, you can become a space explorer too by analyzing the characteristics of all discovered exoplanets (plus some familiar faces like Mars, Saturn, and even Earth). Data fields include planet and host star attributes, discovery methods, and (of course) date of discovery.

Data was originally collected and continues to be updated by Hanno Rein at the Open Exoplanet Catalogue Github repository. 

The aim of this code is to predict the type of planet(TypeFlag) by using the recorded observations(refer columns).
TypeFlag : 0=no known stellar binary companion; 1=P-type binary (circumbinary); 2=S-type binary; 3=orphan planet (no star)


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

###Reading the Data and plotting graphs

```{r}
###Reading the data
data<-read.csv("C:\\R-Programming\\Exoplanet\\oec.csv")
library("reshape2")
library(ggplot2)
##Plotting some graphs for explorations
df.m <- melt(data, "TypeFlag")
g<-ggplot(df.m,aes(x=value,y=TypeFlag))+geom_point(aes(col=TypeFlag))+facet_wrap(~variable)
```

###Including Plots
```{r}
library("reshape2")
##Plotting some graphs for explorations
df.m <- melt(data, "TypeFlag")
g<-ggplot(df.m,aes(x=value,y=TypeFlag))+geom_point(aes(col=TypeFlag))+facet_wrap(~variable)
print(g)

temp<-data
temp$PlanetIdentifier<-NULL
```

###Splitting data

```{r} 
library("caret")
library("caTools")
###Splitting the data into training and testing set
split<-sample.split(temp$TypeFlag,SplitRatio = 0.8)
train<-subset(temp,split==TRUE)
test<-subset(temp,split==FALSE)
temptest<-test
test$TypeFlag<-NULL
```

###Converting the test and train data into Matrices
```{r}
library("xgboost")
library("Matrix")
library(e1071)
library("dplyr")
##Convering factor variables to numeric type
train<- train %>%  mutate_if(is.factor,as.numeric)
test<- test %>%  mutate_if(is.factor,as.numeric)

##Excluding the target variable(dependent variable)
data_variables <- as.matrix(train[,-1])
data_label <- train[,"TypeFlag"]
data_matrix <- xgb.DMatrix(data = data_variables, label = data_label)

numberOfClasses <- length(unique(train$TypeFlag))
xgb_params <- list("objective" = "multi:softmax",eta=0.04,gamma=0.6,max_depth=10,eval_metric="mlogloss","num_class" = numberOfClasses)
xgbcv <- xgb.cv( params = xgb_params, data = data_matrix, nrounds = 400, nfold = 10, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F)


nround    <- xgbcv$best_iteration # number of XGBoost rounds
cv.nfold  <- 10

# Fit cv.nfold * cv.nround XGB models and save OOF predictions
bst_model <- xgb.train(params = xgb_params,
                       data = data_matrix,
                       nrounds = nround)

test_matrix<-xgb.DMatrix(data = as.matrix(test))
```


###Predicting
```{r}
predictionsXGBoost<-predict(bst_model,newdata=test_matrix)
table(predictionsXGBoost,temptest$TypeFlag)
```


#####Getting an accuracy of 99.86% which is huge! (Alternatively, you can use logistic regression and use a less threshhold to increase the sensitivity but by doing this the accuracy will decrease.)